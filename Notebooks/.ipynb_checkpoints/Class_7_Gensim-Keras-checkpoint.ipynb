{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim\n",
    "\n",
    "Gensim is a python library for statistical semantics. It has scallable implementations of:\n",
    "\n",
    "* word2vec\n",
    "* paragraph2vec\n",
    "* Collocation detection\n",
    "* Corpus tools\n",
    "* and much more...\n",
    "\n",
    "Here we will see a simple example on how to train your own word2vec model using gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "import re # Regular expressions package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our word2vec model, we need a corpus. We will use the 'big.txt' file from Peter Norvig blog (http://norvig.com/big.txt). Execute the following commands on the file before continuing:\n",
    "\n",
    "```bash\n",
    "sed -e ':a' -e 'N' -e '$!ba' -e 's/\\n/ /g' big.txt | sed 's/,/ /g' > out.txt\n",
    "```\n",
    "\n",
    "This command will replace every newline and comma with a space, saving the result on `out.txt`.\n",
    "\n",
    "Now let's read, split the sentences and tokenize our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/home/andfre/Downloads/out.txt') as fin:                                                    \n",
    "    raw_text = fin.read()                                                       \n",
    "                                                                                \n",
    "# Get sentences                                                                                                                                                     \n",
    "sentences = re.split('\\?+!+|!+\\?+|\\.+|!+|\\?+', raw_text)                        \n",
    "                                                                                \n",
    "# Get rid of empty sentences                                                    \n",
    "sentences = [s.strip() for s in sentences if len(s.strip()) > 0]                \n",
    "                                                                                \n",
    "# Tokenize sentences (simple space tokenizer) and lower case them               \n",
    "sentences = [[w.lower() for w in s.split()] for s in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our corpus, we can train our word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\n",
    "fname = '/home/andfre/w2v_model'\n",
    "model.save(fname)\n",
    "model = Word2Vec.load(fname)  # you can continue training with the loaded model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences) #Using default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When our model is trained we can perform various syntactic/semantic NLP word tasks, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('law', 0.9024247527122498),\n",
       " ('force', 0.8977823853492737),\n",
       " ('action', 0.8927546143531799),\n",
       " ('constitution', 0.8727369904518127),\n",
       " ('laws', 0.8689889907836914),\n",
       " ('government', 0.868581235408783),\n",
       " ('free', 0.8610345721244812),\n",
       " ('state', 0.8484005331993103),\n",
       " ('events', 0.8425267338752747),\n",
       " ('effect', 0.8419129848480225)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'job'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(['night', 'day', 'job', 'afternoon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84561689182326416"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('man', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.13624771, -0.02266871,  0.63472003, -0.11907553, -0.22339933,\n",
       "       -0.29526153,  0.73241997,  0.21248201, -0.61262256, -0.11415902,\n",
       "       -0.26086268, -1.11159909,  0.61068088, -0.79289532,  0.04550897,\n",
       "        0.33390293, -0.71127284, -0.09242124, -0.33723864,  0.43697152,\n",
       "        0.55743402, -1.02065897, -0.29523399,  0.58874989, -1.2041502 ,\n",
       "        0.64604473,  0.70840406, -0.03391901, -0.10667828,  0.7500543 ,\n",
       "        0.50467169, -0.8855266 ,  0.1187309 ,  0.59485567, -1.5292424 ,\n",
       "       -0.52705014, -0.15584329,  0.32358038,  1.29023087,  0.02368196,\n",
       "       -0.42170233,  0.24598011, -0.15846446, -0.0611285 ,  0.0333156 ,\n",
       "        0.41311353,  0.02710177,  0.96595174, -0.77029204, -0.23648697,\n",
       "        0.41375861, -0.67001677,  0.8149513 , -0.01893596, -0.68569922,\n",
       "        0.42287305,  0.94769877,  1.10255313,  0.00508502,  0.93396974,\n",
       "        0.54887873, -0.71612567,  0.76724315,  0.42328995,  0.34710354,\n",
       "        0.11477941,  0.4066942 , -0.16123356,  0.3841711 , -0.05809061,\n",
       "        0.71795005,  0.02919045, -0.71981007, -0.74605113,  0.108357  ,\n",
       "        0.44586143, -1.04219985,  0.40822962, -0.07151172, -0.87827331,\n",
       "        0.15694752,  0.00341918,  0.13292004, -0.16916652,  0.28713235,\n",
       "       -0.4808785 ,  0.76037967, -0.30213547,  0.67490923, -0.6390723 ,\n",
       "       -0.94279432, -0.48889172,  0.02347166,  0.25461984, -1.62479651,\n",
       "        0.14084665,  0.43721852, -0.53920078,  0.00287561, -0.17538105], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['horse'] # Raw numpy vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are satisfied with our model, we can normalize the vectors and save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.init_sims(replace=True)\n",
    "model.save_word2vec_format('/home/andfre/wordvecs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.Word2Vec.load_word2vec_format('/home/andfre/Downloads/w2v/GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar('power')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "* Download the shakespeare.txt file from Norvig's blog (http://norvig.com/ngrams/shakespeare.txt) and train a word2vec model in the same way we did before.\n",
    "\n",
    "* Search through gensim documentation how to extract the k nearest neighbors of the equation:\n",
    "$$ \\vec{woman} + \\vec{king} - \\vec{man} $$\n",
    "\n",
    "* Compute the KNN for both models (big.txt and shakespeare.txt). What happens? Does one model performs better than the other (by finding the vector 'queen')? Can you explain the disparity between the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PUT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras for NLP\n",
    "\n",
    "Last lecture we learned about Theano and Keras, tools to build and train neural networks. Now let's take a look on how to use these tools for NLP tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "We will only be using Keras' api, but since Keras is built on top of theano, it also inherits it's limitations. One limitation that is very important for NLP tasks is that Theano can only deal with full tensors. That means that you can't have a matrix where the first line has size 20 and the second line has size 30. That's a limitation for NLP because we normally deal with sentences, which are variable in size by nature.\n",
    "\n",
    "A simple workaround for this problem is to pad every sentence with 0s, so they can all have the same size. Keras provides a tool for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  3,  2, 25,  2],\n",
       "       [ 0,  0,  0,  0,  1, 74],\n",
       "       [ 3,  2,  6,  3,  2,  7]], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "sentences = [ # Words indexes\n",
    "             [3, 2, 25, 2],\n",
    "             [1, 74],\n",
    "             [3, 2, 6, 3, 2, 7]\n",
    "             ]\n",
    "pad_sequences(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our sentences can be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer\n",
    "\n",
    "A lot of NLP tasks use word vectors (normally trained using word2vec models or GloVe). To use these word vector in Keras, we need an `Embedding` layer. But first, let's learn how to read pre-trained vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_wordvecs(filename):\n",
    "    fin = open(filename)\n",
    "    \n",
    "    word2index = {}\n",
    "    \n",
    "    # Masking\n",
    "    word2index['MASK'] = 0\n",
    "    # Out of vocabulary words\n",
    "    word2index['UNKNOWN'] = 1\n",
    "    # Padding\n",
    "    word2index['PADDING'] = 2\n",
    "    \n",
    "    word_vecs = []\n",
    "    \n",
    "    for line in fin:\n",
    "        splited_line = line.strip().split()\n",
    "        word = splited_line[0]\n",
    "        word_vecs.append(splited_line[1:])\n",
    "        \n",
    "        word2index[word] = len(word2index)\n",
    "    \n",
    "    word_vecs_np = np.zeros(shape=(len(word2index), len(word_vecs[1])), dtype='float32')\n",
    "    word_vecs_np[3:] = word_vecs\n",
    "    \n",
    "    return word_vecs_np, word2index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a simple call to `read_wordvecs` will return a word vectors matrix and a word to index dictionary.\n",
    "\n",
    "Let's have a look on how to intantiate an Embedding layer then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6, 100)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Word vectors for German words\n",
    "word_vecs, word2index = read_wordvecs('/home/andfre/Downloads/GENSIM_KERAS_CLASS/GermEval/embeddings/GermEval.emb')\n",
    "\n",
    "sentences = [ # Words indexes\n",
    "             [3, 2, 25, 2],\n",
    "             [1, 74],\n",
    "             [3, 2, 6, 3, 2, 7]\n",
    "             ]\n",
    "sentences = pad_sequences(sentences)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "emb_layer = Embedding(output_dim=word_vecs.shape[1], input_dim=word_vecs.shape[0],\n",
    "                      mask_zero=True, weights=[word_vecs], input_length=sentences.shape[1])\n",
    "\n",
    "model.add(emb_layer)\n",
    "\n",
    "model.compile('rmsprop', 'mse')\n",
    "\n",
    "output = model.predict(sentences)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER Classification\n",
    "\n",
    "Now that we learned how to extract and use semantic features from text, let's try a named entity recognition (NER) using these features.\n",
    "\n",
    "Make sure you have the `GermEval` folder before continuing. This folder contains a dataset for German NER and tools to read it. (More info at https://www.ukp.tu-darmstadt.de/fileadmin/user_upload/Group_UKP/publikationen/2014/2014_GermEval_Nested_Named_Entity_Recognition_with_Neural_Networks.pdf)\n",
    "\n",
    "We will use a model that uses a window of size 2 (2 words to the left and 2 words to the right) to predict the NER of the word in the middle. Let's start by reading our dataset. Remember that we already read the word vectors on the last piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from keras.utils import np_utils\n",
    "sys.path.append('/home/andfre/Downloads/GENSIM_KERAS_CLASS')\n",
    "\n",
    "from GermEval import GermEvalReader\n",
    "from GermEval import BIOF1Validation\n",
    "\n",
    "windowSize = 2 # 2 to the left, 2 to the right\n",
    "numHiddenUnits = 100\n",
    "trainFile = '/home/andfre/Downloads/GENSIM_KERAS_CLASS/GermEval/data/NER-de-train.tsv'\n",
    "devFile = '/home/andfre/Downloads/GENSIM_KERAS_CLASS/GermEval/data/NER-de-dev.tsv'\n",
    "testFile = '/home/andfre/Downloads/GENSIM_KERAS_CLASS/GermEval/data/NER-de-test.tsv'\n",
    "\n",
    "# Create a mapping for our labels\n",
    "label2index = {'O':0}\n",
    "idx = 1\n",
    "\n",
    "for bioTag in ['B-', 'I-']:\n",
    "    for nerClass in ['PER', 'LOC', 'ORG', 'OTH']:\n",
    "        for subtype in ['', 'deriv', 'part']:\n",
    "            label2index[bioTag+nerClass+subtype] = idx \n",
    "            idx += 1\n",
    "            \n",
    "#Inverse label mapping\n",
    "index2label = {v: k for k, v in label2index.items()}\n",
    "\n",
    "# Read in data   \n",
    "train_sentences = GermEvalReader.readFile(trainFile)\n",
    "dev_sentences = GermEvalReader.readFile(devFile)\n",
    "test_sentences = GermEvalReader.readFile(testFile)\n",
    "\n",
    "# Create numpy arrays\n",
    "train_x, train_y = GermEvalReader.createNumpyArray(train_sentences, windowSize, word2index, label2index)\n",
    "dev_x, dev_y = GermEvalReader.createNumpyArray(dev_sentences, windowSize, word2index, label2index)\n",
    "test_x, test_y = GermEvalReader.createNumpyArray(test_sentences, windowSize, word2index, label2index)\n",
    "\n",
    "# Train_y is a 1-dimensional vector containing the index of the label\n",
    "# With np_utils.to_categorical we map it to a 1 hot matrix\n",
    "n_out = len(label2index)\n",
    "train_y_cat = np_utils.to_categorical(train_y, n_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our data. Now we will build our model:\n",
    "\n",
    "* An Embedding Layer\n",
    "* A LSTM Layer\n",
    "* A Dense Layer (Softmax activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.regularizers import l2\n",
    "\n",
    "n_in = 2*windowSize+1\n",
    "n_hidden = numHiddenUnits\n",
    "n_out = len(label2index)\n",
    "\n",
    "number_of_epochs = 10\n",
    "batch_size = 35\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(output_dim=word_vecs.shape[1], input_dim=word_vecs.shape[0],\n",
    "                    input_length=n_in,  weights=[word_vecs], mask_zero=False))  \n",
    "\n",
    "model.add(LSTM(n_hidden, W_regularizer=l2(0.0001), U_regularizer=l2(0.0001)))\n",
    "\n",
    "model.add(Dense(n_out, activation='softmax', W_regularizer=l2(0.0001)))\n",
    "            \n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our model compiled, the only thing left to do is train.\n",
    "\n",
    "Since we don't want to validate using Keras' default validation (accuracy or loss), but BIOF1, we will train the model for 1 epoch, validate, and then loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452830 train samples\n",
      "5 train dimension\n",
      "96483 test samples\n",
      "\n",
      "10 epochs\n",
      "12938 mini batches\n",
      "\n",
      "A little bit too much for the lecture. Using 10k samples instead\n",
      "\n",
      "104.38 sec for training\n",
      "1 epoch: F1 on dev: 0.094899, F1 on test: 0.116915\n",
      "79.00 sec for training\n",
      "2 epoch: F1 on dev: 0.263254, F1 on test: 0.287046\n",
      "75.29 sec for training\n",
      "3 epoch: F1 on dev: 0.339383, F1 on test: 0.370438\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7e703d8275da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#Train for 1 epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%.2f sec for training\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1125\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    843\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "print(str(train_x.shape[0]) + ' train samples')\n",
    "print(str(train_x.shape[1]) + ' train dimension')\n",
    "print(str(test_x.shape[0]) + ' test samples')\n",
    "\n",
    "print(\"\\n%d epochs\" % number_of_epochs)\n",
    "print(\"%d mini batches\" % (len(train_x)/batch_size))\n",
    "\n",
    "print(\"\\nA little bit too much for the lecture. Using 10k samples instead\\n\")\n",
    "train_x = train_x[:10000]\n",
    "dev_x = dev_x[:10000]\n",
    "test_x = test_x[:10000]\n",
    "train_y_cat = train_y_cat[:10000]\n",
    "dev_y = dev_y[:10000]\n",
    "test_y = test_y[:10000]\n",
    "\n",
    "sys.stdout.flush()\n",
    "\n",
    "for epoch in range(number_of_epochs):    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #Train for 1 epoch\n",
    "    model.fit(train_x, train_y_cat, nb_epoch=1, batch_size=batch_size, verbose=False, shuffle=True)   \n",
    "    print(\"%.2f sec for training\" % (time.time() - start_time))\n",
    "    sys.stdout.flush()\n",
    "  \n",
    "    # Compute precision, recall, F1 on dev & test data\n",
    "    pre_dev, rec_dev, f1_dev = BIOF1Validation.compute_f1(model.predict_classes(dev_x, verbose=0), dev_y, index2label)\n",
    "    pre_test, rec_test, f1_test = BIOF1Validation.compute_f1(model.predict_classes(test_x, verbose=0), test_y, index2label)\n",
    "\n",
    "    print(\"%d epoch: F1 on dev: %f, F1 on test: %f\" % (epoch+1, f1_dev, f1_test))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "* Using the following code, load the imdb sentiment analysis dataset.\n",
    "\n",
    "```python\n",
    "from keras.datasets import imdb\n",
    "\n",
    "#Using 20k most frequent words\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=20000,\n",
    "                                                      test_split=0.2)\n",
    "```\n",
    "\n",
    "* Pad X_train and X_test using keras preprocessing tools. It's a good idea to set the `maxlen` parameter to something around 80 when padding sentences.\n",
    "* Build a model with an Embedding layer at the beggining and a binary output (you can use LSTM or GRU as hidden layers). The Embedding layer shouldn't be initialized (just omit the `weights` parameter)\n",
    "* Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PUT YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
