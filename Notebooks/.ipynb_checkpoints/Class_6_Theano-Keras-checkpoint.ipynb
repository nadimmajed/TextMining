{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano\n",
    "\n",
    "This notebook is heavily influenced by the tutorial at [https://github.com/UKPLab/deeplearning4nlp-tutorial]()\n",
    "\n",
    "### Advantages\n",
    "\n",
    "* Python library with tight integration of NumPy\n",
    "    * Easy syntax for matrix operations\n",
    "* Transparent use of GPU (speed-up of up to 140x)\n",
    "* Efficient symbolic differentiation (Theano computes the gradient)\n",
    "* Speed and stability optimizations\n",
    "* Calculations are dynamically mapped to C code\n",
    "    * We do our computations as fast as we would have written it in C\n",
    "    * Great performance (>10 faster than Java in my experiments)\n",
    "    \n",
    "### Disadvantages\n",
    "\n",
    "* Debugging is really hard\n",
    "\n",
    "### Some notes\n",
    "\n",
    "* Theano utilizes BLAS (Basic Linear Algebra Subprograms)\n",
    "    * Building blocks for fast vector and matrix operations\n",
    "    * Often written in Fortran, sometimes in Assembler\n",
    "* For performance optimization install a BLAS package\n",
    "* Benchmark different BLAS packages\n",
    "* I use a manually compiled OpenBlas implementation\n",
    "    * Installation notes: http://deeplearning.net/software/theano/install_ubuntu.html\n",
    "    \n",
    "### Theano's Flow\n",
    "\n",
    "Here are the execution steps for a Theano script:\n",
    "\n",
    "![Executions steps for a Theano script](files/theano_pipeline.png)\n",
    "\n",
    "### Defining a computation graph\n",
    "\n",
    "Let's say we want to define a computation graph for the equation $z = a + b$, where $a$, $b$ and $z$ are scalars.\n",
    "\n",
    "First of all, let's import Theano and the `tensor` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define our graph's inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = T.scalar() #T.scalar() defines a float variable\n",
    "b = T.scalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](files/graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling and Using a Theano Function\n",
    "\n",
    "Now that we defined the computation graph for our equation, we can compile it to C code (or CUDA, depending on Theano's configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = theano.function(inputs=[a, b], outputs=z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use the function as any other python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 3 = 5.0\n",
      "1 + 1 = 2.0\n",
      "2.5 + 8.1 = 10.6\n"
     ]
    }
   ],
   "source": [
    "print(\"2 + 3 = {}\".format(f(2, 3)))\n",
    "print(\"1 + 1 = {}\".format(f(1, 1)))\n",
    "print(\"2.5 + 8.1 = {}\".format(f(2.5, 8.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Variables\n",
    "\n",
    "Sometimes we need our function to have some internal state. These are implemented using __Shared Variables__.\n",
    "\n",
    "Let's implement a simple accumulator using a shared variable and the __updates__ mechanism.\n",
    "\n",
    "First, define the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_state = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, instantiate your shared variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Defining a name is good for debugging\n",
    "state = theano.shared(value=initial_state, name='state')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a function that updates the state every time it's called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "next_state = state + 1\n",
    "accum = theano.function(inputs=[], outputs=next_state,\n",
    "                        updates={state: next_state})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(accum())\n",
    "print(accum())\n",
    "print(accum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we define the `state` variable as a shared variable, it is internally available to Theano. If we define another function that uses the same shared variable, both functions will share a common internal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "x = T.iscalar() #T.iscalar defines an int variable\n",
    "\n",
    "next_state = state + x\n",
    "\n",
    "accum_2 = theano.function(inputs=[x], outputs=next_state,\n",
    "                          updates={state: next_state})\n",
    "\n",
    "print(accum_2(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation\n",
    "\n",
    "One of the great things about Theano is that you can automatically differentiate a computation graph in respect to some variable (shared or not). As an example let's compile a function that computes the derivative of $x^2$ at a given point.\n",
    "\n",
    "Just as reminder:\n",
    "\n",
    "$$\\frac{d x^2}{d x} = 2x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "x = T.scalar()\n",
    "\n",
    "z = x ** 2\n",
    "\n",
    "deriv = theano.grad(z, wrt=x)\n",
    "\n",
    "f = theano.function(inputs=[x], outputs=deriv)\n",
    "\n",
    "print(f(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Neuron Implementation\n",
    "\n",
    "Let's implement the simple function $c = a \\wedge b$. But here is the catch: let's implement it by training a sigmoid neuron.\n",
    "\n",
    "First thing we need is to define our training data, which will be the following examples:\n",
    "\n",
    "| $x_0$ | $x_1$ | $y$ |\n",
    "|---|---|---|\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 0 |\n",
    "| 1 | 0 | 0 |\n",
    "| 1 | 1 | 1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_data = np.asarray([\n",
    "                [0, 0],\n",
    "                [0, 1],\n",
    "                [1, 0],\n",
    "                [1, 1]\n",
    "                ], dtype='int32')\n",
    "y_data = np.asarray([0, 0, 0, 1], dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need a weight vector for our neuron and a bias. We will initialize this vector and the bias with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_w = np.zeros(shape=(2,1))\n",
    "init_b = 0.\n",
    "w = theano.shared(value=init_w, name='weights')\n",
    "b = theano.shared(value=init_b, name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]]\n"
     ]
    }
   ],
   "source": [
    "x = T.imatrix() #T.imatrix() defines a matrix of ints\n",
    "\n",
    "z = x.dot(w) + b\n",
    "out = 1 / (1 + T.exp(-z)) #sigmoid activation\n",
    "\n",
    "feedforward = theano.function(inputs=[x], outputs=out)\n",
    "\n",
    "print feedforward(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined a feedforward function. Let's now define a training function using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andfre/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:14: UserWarning: The parameter 'updates' of theano.function() expects an OrderedDict, got <type 'dict'>. Using a standard dictionary here results in non-deterministic behavior. You should use an OrderedDict if you are using Python 2.7 (theano.compat.OrderedDict for older python), or use a list of (shared, update) pairs. Do not just convert your dictionary to this type before the call as the conversion will still be non-deterministic.\n"
     ]
    }
   ],
   "source": [
    "y = T.ivector() #T.ivector() defines a vector of ints\n",
    "\n",
    "cost = ((y - out.T) ** 2).sum()\n",
    "\n",
    "grad_w = theano.grad(cost, wrt=w)\n",
    "grad_b = theano.grad(cost, wrt=b)\n",
    "\n",
    "weight_updates = {\n",
    "           w: w - grad_w, #learning rate = 1.0\n",
    "           b: b - grad_b\n",
    "           }\n",
    "\n",
    "train = theano.function(inputs=[x, y], outputs=None,\n",
    "                        updates=weight_updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our train function to fit our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.84705146e-06]\n",
      " [  1.14550430e-02]\n",
      " [  1.14550430e-02]\n",
      " [  9.86431059e-01]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000): #10k epochs\n",
    "    train(x_data, y_data)\n",
    "print(feedforward(x_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Classification\n",
    "\n",
    "Now a more fast-paced implementation of a MLP to classify the MNIST dataset. Our network will have:\n",
    "\n",
    "* An input layer of size $28 \\times 28 = 786$ neurons\n",
    "* A hidden layer\n",
    "$$ hidden(x) = \\sigma(W \\cdot x + b) $$\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "\n",
    "* A softmax layer\n",
    "$$ softmax(x) = \\psi(W_{softmax} \\cdot x + b_{softmax}) $$\n",
    "$$ \\psi(z_i) = \\frac{e^{z_i}}{\\sum_{z_j \\in z} e^{z_j}} $$\n",
    "\n",
    "We will implement 3 classes:\n",
    "\n",
    "* HiddenLayer\n",
    "* SoftmaxLayer\n",
    "* MLP\n",
    "\n",
    "#### HiddenLayer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self, inp, n_in, n_out):\n",
    "        \"\"\"\n",
    "        inp: Theano variable for the input\n",
    "        n_in: Number of incoming units\n",
    "        n_out: Number of outgoing units\n",
    "        \"\"\"\n",
    "        #Initialize weights\n",
    "        init_W = np.random.normal(size=(n_in, n_out))\n",
    "        init_b = np.zeros(shape=(n_out,))\n",
    "        W = theano.shared(value=init_W, name='hidden_w')\n",
    "        b = theano.shared(value=init_b, name='hidden_b')\n",
    "        \n",
    "        self.params = [W, b]\n",
    "        \n",
    "        z = inp.dot(W) + b\n",
    "        self.out = 1 / (1 + T.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SoftmaxLayer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SoftmaxLayer(object):\n",
    "    def __init__(self, inp, n_in, n_out):\n",
    "        \"\"\"\n",
    "        inp: Theano variable for the input\n",
    "        n_in: Number of incoming units\n",
    "        n_out: Number of outgoing units\n",
    "        \"\"\"\n",
    "        #Initialize weights\n",
    "        init_W = np.random.normal(size=(n_in, n_out))\n",
    "        init_b = np.zeros(shape=(n_out,))\n",
    "        W = theano.shared(value=init_W, name='softmax_w')\n",
    "        b = theano.shared(value=init_b, name='softmax_b')\n",
    "        \n",
    "        self.params = [W, b]\n",
    "        \n",
    "        z = inp.dot(W) + b\n",
    "        self.p_y_given_x = T.nnet.softmax(z)\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        \n",
    "    def neg_log_likelihood(self, y):\n",
    "        \"\"\"\n",
    "        Computes the negative log-likelihood. The function explained:\n",
    "        \n",
    "        T.log(self.p_y_given_x): Compute the negative log-likelihood of p_y_given_x\n",
    "        [T.arange(y.shape[0]), y]: Select the neuron at position y, our label\n",
    "        T.mean(): Compute the average over our mini batch\n",
    "        \"\"\"\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "     def __init__(self,inp, n_in, n_hidden, n_out):\n",
    "        \"\"\"\n",
    "        :param inp: Input variable (the data)\n",
    "        :param n_in: Input dimension\n",
    "        :param n_hidden: Hidden size\n",
    "        :param n_out: Output size\n",
    "        \"\"\"\n",
    "        self.hiddenLayer = HiddenLayer(inp=inp, n_in=n_in, n_out=n_hidden)\n",
    "        \n",
    "        self.softmaxLayer = SoftmaxLayer(inp=self.hiddenLayer.out, n_in=n_hidden, n_out=n_out)\n",
    "        \n",
    "        #Negative log likelihood of this MLP = neg. log likelihood of softmax layer\n",
    "        self.neg_log_likelihood = self.softmaxLayer.neg_log_likelihood\n",
    "        \n",
    "        #Parameters of this MLP = Parameters offen Hidden + SoftmaxLayer\n",
    "        self.params = self.hiddenLayer.params + self.softmaxLayer.params   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Code\n",
    "\n",
    "Now we have classes to build a computation graph, we want to train the internal states (the weights) using the MNIST dataset. The following code takes care of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_x-Matrix:  (50000, 784)\n",
      "Shape of train_y-vector:  (50000,)\n",
      "Shape of dev_x-Matrix:  (10000, 784)\n",
      "Shape of test_x-Matrix:  (10000, 784)\n",
      ">> train- and feedforwad-functions are compiled <<\n"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "\n",
    "# Load the pickle file for the MNIST dataset.\n",
    "dataset = '/home/andfre/Downloads/mnist.pkl.gz'\n",
    "\n",
    "f = gzip.open(dataset, 'rb')\n",
    "train_set, dev_set, test_set = cPickle.load(f)\n",
    "f.close()\n",
    "\n",
    "#train_set contains 2 entries, first the X values, second the Y values\n",
    "train_x, train_y = train_set\n",
    "dev_x, dev_y = dev_set\n",
    "test_x, test_y = test_set\n",
    "\n",
    "#Created shared variables for these sets (for performance reasons)\n",
    "train_x_shared = theano.shared(value=np.asarray(train_x, dtype='float32'), name='train_x')\n",
    "train_y_shared = theano.shared(value=np.asarray(train_y, dtype='int32'), name='train_y')\n",
    "\n",
    "\n",
    "print \"Shape of train_x-Matrix: \",train_x_shared.get_value().shape\n",
    "print \"Shape of train_y-vector: \",train_y_shared.get_value().shape\n",
    "print \"Shape of dev_x-Matrix: \",dev_x.shape\n",
    "print \"Shape of test_x-Matrix: \",test_x.shape\n",
    "\n",
    "###########################\n",
    "#\n",
    "# Start to build the model\n",
    "#\n",
    "###########################\n",
    "\n",
    "# Hyper parameters\n",
    "hidden_units = 50\n",
    "learning_rate = 0.01\n",
    "batch_size = 20\n",
    "\n",
    "# Variables for our network\n",
    "index = T.lscalar()  # index to a minibatch\n",
    "x = T.fmatrix('x')  # the data, one image per row\n",
    "y = T.ivector('y')  # the labels are presented as 1D vector of [int] labels\n",
    "\n",
    "np.random.seed(1234) #To have deterministic results\n",
    "\n",
    "# construct the MLP class\n",
    "classifier = MLP(inp=x, n_in=28 * 28, n_hidden=50, n_out=10)\n",
    "\n",
    "# Define our cost function = error function\n",
    "cost = classifier.neg_log_likelihood(y) #Here we could add L1 and L2 terms for regularization\n",
    "\n",
    "# Update param := param - learning_rate * gradient(cost, param)\n",
    "grads = [T.grad(cost, param) for param in classifier.params]\n",
    "updates = [(param, param - learning_rate * grad ) \n",
    "           for param, grad in zip(classifier.params, grads)]\n",
    "\n",
    "# Now create a train function\n",
    "# The train function needs the data, the index for the minibatch and the updates to work correctly\n",
    "train_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_x_shared[index * batch_size: \\\n",
    "                              (index + 1) * batch_size],\n",
    "            y: train_y_shared[index * batch_size: \\\n",
    "                              (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Create a prediction function\n",
    "feedforward = theano.function(inputs=[x], outputs=classifier.softmaxLayer.y_pred)\n",
    "\n",
    "print \">> train- and feedforwad-functions are compiled <<\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with a train and a feedforward function, we can finally train our weights for the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 mini batches\n",
      "10 epochs\n",
      "0 epoch: Accuracy on dev: 0.518700, accuracy on test: 0.523800\n",
      "1 epoch: Accuracy on dev: 0.660900, accuracy on test: 0.658100\n",
      "2 epoch: Accuracy on dev: 0.723300, accuracy on test: 0.717700\n",
      "3 epoch: Accuracy on dev: 0.757000, accuracy on test: 0.749700\n",
      "4 epoch: Accuracy on dev: 0.781200, accuracy on test: 0.774000\n",
      "5 epoch: Accuracy on dev: 0.797300, accuracy on test: 0.790800\n",
      "6 epoch: Accuracy on dev: 0.810100, accuracy on test: 0.804300\n",
      "7 epoch: Accuracy on dev: 0.818700, accuracy on test: 0.813800\n",
      "8 epoch: Accuracy on dev: 0.828200, accuracy on test: 0.822600\n",
      "9 epoch: Accuracy on dev: 0.834800, accuracy on test: 0.829500\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "number_of_minibatches = len(train_x) / batch_size\n",
    "print \"%d mini batches\" % (number_of_minibatches)\n",
    "\n",
    "number_of_epochs = 10\n",
    "print \"%d epochs\" % number_of_epochs\n",
    "\n",
    "#\n",
    "def compute_accuracy(dataset_x, dataset_y): \n",
    "    predictions = feedforward(dataset_x)\n",
    "    errors = sum(predictions != dataset_y) #Number of errors\n",
    "    accuracy = 1 - errors/float(len(dataset_y))\n",
    "    return accuracy\n",
    "\n",
    "for epoch in xrange(number_of_epochs):\n",
    "    #Train the model on all mini batches\n",
    "    for idx in xrange(0, number_of_minibatches):\n",
    "        train_model(idx)\n",
    " \n",
    "\n",
    "    accuracy_dev = compute_accuracy(dev_x, dev_y)\n",
    "    accuracy_test = compute_accuracy(test_x, test_y)\n",
    "\n",
    "    print \"%d epoch: Accuracy on dev: %f, accuracy on test: %f\" % (epoch, accuracy_dev, accuracy_test)\n",
    "    \n",
    "print \"DONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "* Try changing the network's hyper-parameters. Try to increase the accuracy by doing that.\n",
    "* Add (squared) L2 regularization to the training (good values for L2 regularization are between 1e-3 to 1e-7). Reminder:\n",
    "$$ reg\\_cost = cost + \\lambda \\times (\\sum_{w \\in \\theta} w^2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "Keras is a deep learning framework that serves as an abstraction layer for Theano, providing a variety of deep learning techniques.\n",
    "\n",
    "Let's implement the same MLP as before, but now using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#Passing the y data to one-hot encoding\n",
    "#Is we use sparse encoding, the Keras accuracy will be buggy\n",
    "keras_train_y = np_utils.to_categorical(train_y)\n",
    "keras_dev_y = np_utils.to_categorical(dev_y)\n",
    "\n",
    "#Building and training the model\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_units, input_shape=(784,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=learning_rate),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_x, keras_train_y, validation_data=(dev_x, keras_dev_y),\n",
    "          batch_size=batch_size, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Now search through Keras' documentation to complete the following exercises:\n",
    "\n",
    "* Try changing the network's hyper-parameters. Try to increase the accuracy by doing that.\n",
    "* Add (squared) L2 regularization to the training."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
