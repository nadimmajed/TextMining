{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"import pyowm ;owm=pyowm.OWM('e692337a0e1be1f73b23bf38bd19275b') ;fc=owm.daily_forecast('New York City,US',limit=4) ; f=fc.get_forecast();lst = f.get_weathers()\\nfor weather in lst: print(weather.get_reference_time('iso'),weather.get_status(),weather.get_temperature('celsius'))\", \"import pyowm ;owm=pyowm.OWM('e692337a0e1be1f73b23bf38bd19275b');obs=owm.weather_at_place('New York City,US') ;obs.get_reception_time(timeformat='iso');w=obs.get_weather() ;print(w.get_status(),w.get_temperature('celsius'),w.get_wind(),w.get_pressure())\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Query inputting\n",
    "\n",
    "Text1=open(\"/home/nadim/Desktop/Text-mining-project/query.txt\",\"r\")\n",
    "text=Text1.read()\n",
    "queries=text.split(\"\\n\")\n",
    "q=20\n",
    "\n",
    "# Feature extracting/Syntaxic Tree\n",
    "\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "output = nlp.annotate(text, properties={\n",
    "  'annotators': 'tokenize,ssplit,pos,depparse,parse,dcoref,ner',\n",
    "  'outputFormat': 'json'\n",
    "  })\n",
    "syntaxic_output=[output['sentences'][i]['parse'] for i in range(0,len(output['sentences']))]\n",
    "\n",
    "\n",
    "trimmed_output=[syntaxic_output[i].replace(\" \",\"\")  for i in range(0,len(syntaxic_output))]\n",
    "trimmed_output=[trimmed_output[i].replace(\"(\",\"\")  for i in range(0,len(syntaxic_output))]\n",
    "trimmed_output=[trimmed_output[i].replace(\")\",\" \")  for i in range(0,len(syntaxic_output))]\n",
    "\n",
    "\n",
    "\n",
    "# SRL: looking for possible params and functions (NP pos for parameters and VP for functions)\n",
    "\n",
    "splitted_queries=[trimmed_output[i].split(\"\\n\") for i in range(0,len(trimmed_output))]\n",
    "possible_funcs=[[splitted_queries[j][i] for i in range(0,len(splitted_queries[j])) if splitted_queries[j][i].find(\"VP\")!=-1] for j in range(0,len(splitted_queries))]\n",
    "possible_funcs=[[a[a.find(\"VP\")+2:-2] for a in possible_funcs[i] ] for i in range(0,len(possible_funcs)) ]\n",
    "possible_funcs=[[a.strip('`') for a in possible_funcs[i] ] for i in range(0,len(possible_funcs)) ]\n",
    "possible_params=[[splitted_queries[j][i] for i in range(0,len(splitted_queries[j])) if splitted_queries[j][i].find(\"NP\")!=-1] for j in range(0,len(splitted_queries))]\n",
    "possible_params=[[a[a.find(\"NP\")+2:-2] for a in possible_params[i] if a[a.find(\"NP\")+2:-2]!=\"\" ] for i in range(0,len(possible_params)) ]\n",
    "possible_params=[[a.replace(\"`\",\"\") for a in possible_params[i] ] for i in range(0,len(possible_params)) ]\n",
    "possible_params=[[a.replace(\"'\",\"\") for a in possible_params[i] ] for i in range(0,len(possible_params)) ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "     return any(char.isdigit() for char in inputString)\n",
    "\n",
    "\n",
    "\n",
    "tags=open(\"/home/nadim/Desktop/Text-mining-project/POS_TAGS.txt\",\"r\")\n",
    "tags=tags.read()\n",
    "tags=tags.split(\"\\n\")\n",
    "\n",
    "\n",
    "for i in range(0, len(tags)):\n",
    "    possible_params=[[a.replace(tags[i],\"\").strip() for a in possible_params[j] ] for j in range(0,len(possible_params))] \n",
    "    possible_funcs=[[a.replace(tags[i],\"\").strip() for a in possible_funcs[j] ] for j in range(0,len(possible_funcs))]\n",
    "\n",
    "for i in range(0,len(possible_params)):\n",
    "    if (possible_params[i][0]==output['sentences'][i]['tokens'][0]['lemma']):\n",
    "        possible_funcs[i].extend([possible_params[i][0]])\n",
    "        possible_params[i].remove(possible_params[i][0])    \n",
    "    for j in range(0,len(possible_params[i])):\n",
    "        if hasNumbers(possible_params[i][j]):\n",
    "            a=possible_params[i][j].split()\n",
    "            possible_params[i].remove(possible_params[i][j])\n",
    "            possible_params[i]=a+possible_params[i]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# For SM later (only the possible parameters because we don't functions in parameters matching)\n",
    "possible_parameters=[possible_params[i][:] for i in range(0,len(possible_params))] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# For guessing the right api method\n",
    "for i in range(0,len(possible_funcs)):\n",
    "    if possible_funcs[i]!=[] :\n",
    "        if possible_funcs[i][0]!='get':\n",
    "            possible_params[i].append(possible_funcs[i][0]) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loading the acktionKB (methods and their parameters)\n",
    "\n",
    "import json\n",
    "actionKB=json.loads(open(\"/home/nadim/Desktop/Text-mining-project/actionkb.json\").read())\n",
    "functions=[actionKB[i][\"NAME\"].replace(\"_\",\" \") for i in range(0,len(actionKB))]\n",
    "functions_desc=[actionKB[i][\"Desc\"] for i in range(0,len(actionKB))]\n",
    "\n",
    "parameter=[actionKB[i][\"PARAMS\"] for i in range(0,len(actionKB))]\n",
    "#parameters=[[parameter[i][j][\"Name\"] for j in range(0,len(parameter[i]))] for i in range(0,len(parameter))]\n",
    "parameters=[[parameter[i][j][\"Desc\"] for j in range(0,len(parameter[i]))] for i in range(0,len(parameter))] \n",
    "\n",
    "for i in range(0,len(functions)):\n",
    "    if functions[i]!='get' :parameters[i].append(functions[i]) \n",
    "    parameters[i].append(functions_desc[i])\n",
    "\n",
    "\n",
    "\n",
    "# Preparing the pair lists for the similarity computing using the word2vec model trained on wiki-2014 corpora\n",
    "\n",
    "pairs_model='\"pairs\": [{\\n        \"t2\": \"dd\",\\n        \"t1\": \"db\"\\n    }end'\n",
    "add_on=', {\\n        \"t2\": \"dd\",\\n        \"t1\": \"db\"\\n    }end'\n",
    "pairs_list=[]\n",
    "for k in range(0,len(parameters)):\n",
    "    pairss=[]\n",
    "    for i in range(0,len(possible_params[q])):\n",
    "        pairs=pairs_model\n",
    "        for j in range(0,len(parameters[k])):\n",
    "            pairs=pairs.replace(\"db\",possible_params[q][i])\n",
    "            pairs=pairs.replace(\"dd\",parameters[k][j])\n",
    "            if j!=len(parameters[k])-1 :\n",
    "                pairs=pairs.replace(\"end\",add_on)\n",
    "            else:\n",
    "                pairs=pairs.replace(\"end\",\"]\\n}\")\n",
    "        pairss.append(pairs)\n",
    "    pairs_list.append(pairss)\n",
    "\n",
    "\n",
    "\n",
    "# Invoke the INDRA server for Similarity calculating (Word2Vec Model)\n",
    "\n",
    "import http.client\n",
    "\n",
    "conn = http.client.HTTPConnection(\"alphard.fim.uni-passau.de:8916\",timeout=50)\n",
    "\n",
    "headers = {\n",
    "    'accept': \"application/json\",\n",
    "    'content-type': \"application/json\",\n",
    "    'authorization':  \"Basic aW5kcmE6UVk4SDVkcm9ZODQ9\",\n",
    "    'cache-control': \"no-cache\"\n",
    "}\n",
    "values=[]\n",
    "for i in range(0,len(pairs_list)):\n",
    "    value=[]\n",
    "    for j in range(0,len(pairs_list[i])):\n",
    "        payload='{\\n    \"corpus\": \"wiki-2014\",\\n    \"model\": \"W2V\",\\n    \"language\": \"EN\",\\n    \"scoreFunction\": \"COSINE\",\\n    '+pairs_list[i][j]+'\\n'\n",
    "        conn.request(\"POST\", \"/relatedness\", payload, headers)\n",
    "        res = conn.getresponse()\n",
    "        data = res.read()\n",
    "        evalued=json.loads(data.decode(\"utf-8\"))\n",
    "        value.append(evalued['pairs'])\n",
    "    values.append(value)\n",
    "\n",
    "\n",
    "# Calculating the score of each function with the query and sort them\n",
    "\n",
    "scores=[]\n",
    "for i in range(0,len(values)):\n",
    "    s=0\n",
    "    for j in range(0,len(values[i])):\n",
    "        a=0\n",
    "        for k in range(0,len(values[i][j])):\n",
    "            a=a+values[i][j][k]['score']\n",
    "        s=s+(a/len(values[i][j]))\n",
    "    scores.append((functions[i],str(s/len(values[i])),i))\n",
    "\n",
    "def getKey(item):\n",
    "    return item[1]\n",
    "scores=sorted(scores, key=getKey,reverse=True)\n",
    "\n",
    "\n",
    "# Defining precision of function selection (varying Epsilon)\n",
    "epsilon=0.08\n",
    "Max=float(scores[0][1])\n",
    "left=0\n",
    "right=len(scores)\n",
    "while(right-left>1):\n",
    "    m=int((left+right)/2)\n",
    "    if (Max-float(scores[m][1]))<epsilon :\n",
    "        left=m\n",
    "    else :\n",
    "        right=m\n",
    "candidates=scores[0:left+1]\n",
    "\n",
    "\n",
    "# Semantic matching using our proposed processing ( NER-Similarity-Formatting)\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "NER = StanfordNERTagger('/home/nadim/Desktop/Text-mining-project/english.all.3class.caseless.distsim.crf.ser.gz','/home/nadim/Downloads/stanford-ner-2015-12-09/stanford-ner.jar',encoding='utf-8')\n",
    "\n",
    "\"\"\" This in order to ignore the warnings\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "    \n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "def getcode(): # method that will return python code to execute\n",
    "    global param\n",
    "    global code\n",
    "    code=[]\n",
    "    for f in candidates:\n",
    "        possible_parametersT=[possible_parameters[i][:] for i in range(0,len(possible_parameters))] \n",
    "        init_code=actionKB[f[2]]['instantiation']\n",
    "        for i in range(0,len(actionKB[f[2]]['PARAMS'])):\n",
    "            paramnotfound=True\n",
    "            counter=0\n",
    "            \n",
    "            while (paramnotfound==True) and (counter<len(possible_parametersT[q])):\n",
    "                counter+=1\n",
    "                if ('NER' in actionKB[f[2]]['PARAMS'][i]):\n",
    "                    #NER tagging\n",
    "                    tag=actionKB[f[2]]['PARAMS'][i]['NER']\n",
    "                    for j in range(0,len(possible_parametersT[q])):\n",
    "                        token= word_tokenize(possible_parametersT[q][j])\n",
    "                        tagged=[a[1] for a in NER.tag(token)]\n",
    "                        if tag in tagged:\n",
    "                            param=' '.join([token[h] for h in range(0,len(tagged)) if tagged[h]==tag])\n",
    "                            paramnotfound=False\n",
    "                            possible_parametersT[q].remove(possible_parametersT[q][j])\n",
    "                            break\n",
    "                else:\n",
    "                    #similarity\n",
    "                    param_scores=[]\n",
    "                    for j in range(0,len(possible_parametersT[q])):\n",
    "                        for a in values[f[2]][j]:\n",
    "                            if a['t2']==actionKB[f[2]]['PARAMS'][i]['Desc']: param_scores.insert(-1,(a['score'],j))\n",
    "                    param=possible_parametersT[q][sorted(param_scores,key=getKey,reverse=True)[0][1]]\n",
    "                    #possible_parameters[q].remove(param)\n",
    "                    paramnotfound=False\n",
    "                if ('Format' in actionKB[f[2]]['PARAMS'][i]):\n",
    "                    #Formatting\n",
    "                    Format=actionKB[f[2]]['PARAMS'][i]['Format'].split(',')\n",
    "                    if  Format[0]=='Number':\n",
    "                        #assign number and check the preselected param\n",
    "                        if is_number(param)==False:\n",
    "                            for p in possible_parametersT[q]:\n",
    "                                if is_number(p): \n",
    "                                    param=p\n",
    "                                    break\n",
    "                    elif Format[0]==\"''\":\n",
    "                        param=query[q][query[q].find('\"'):query[q].rfind('\"')+1]\n",
    "                    else:\n",
    "                        DB=json.loads(open(\"/home/nadim/Desktop/Text-mining-project/\"+actionKB[f[2]]['PARAMS'][i]['Data']).read())\n",
    "                        check=False\n",
    "                        for a in DB: \n",
    "                             if param in a.values():\n",
    "                                check=True\n",
    "                                break\n",
    "                        if check==False:\n",
    "                            paramnotfound=True\n",
    "                            continue\n",
    "                        elif param in [a[Format[0]] for a in DB]:\n",
    "                            index=[a[Format[0]] for a in DB].index(param)\n",
    "                            param=actionKB[f[2]]['PARAMS'][i]['Format'].replace(Format[0],param)\n",
    "                        else:\n",
    "                            for k in range(0,len(DB)):\n",
    "                                if param in DB[k].values():\n",
    "                                    index=k\n",
    "                                    break\n",
    "                            param=actionKB[f[2]]['PARAMS'][i]['Format'].replace(Format[0],DB[index][Format[0]])\n",
    "                        if len(Format)>1:\n",
    "                            for l in range(1,len(Format)):\n",
    "                                param=param.replace(Format[l],DB[index][Format[l]])\n",
    "            init_code=init_code.replace(actionKB[f[2]]['PARAMS'][i]['Name'],param)\n",
    "        code.append(init_code)    \n",
    "    return code\n",
    "\n",
    "\n",
    "\n",
    "codes=getcode()\n",
    "print(codes)\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 3, in <module>\n",
      "NameError: name 'codes' is not defined\n"
     ]
    }
   ],
   "source": [
    "%%python2\n",
    "# Executing of the code\n",
    "import sys\n",
    "for i in range(0,len(codes)):\n",
    "    print(\"Executing...\",candidates[i][0])\n",
    "    try:\n",
    "        exec(codes[i])\n",
    "    except:\n",
    "        print(\"Something went wrong: \"+str(sys.exc_info()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
